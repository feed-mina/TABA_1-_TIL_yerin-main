TLB 는 바이트 단위로 저장한다. 사이즈가 굉장히 큰편이다. 메모리 사이즈보다는 작지만 그럼에도 큰편이다.
엔트리갯수가 100개 ? 몇개 안돼

TLB사이즈와 캐쉬를 할때 TLB엔트리가 왜이렇게 작지? 이유는 엔트리하나에 64바이트이기 때문에
4키로바이트 영억을 엔트리 하나로 할 수 있다
엔트리 수를 줄여놓은 이유 : 엔트리 하나가 용량이 크기 때문에

사용자가 쓰는 address 는 모두 VMaddress
PM address에 second level ,
pap에 나머지 옵션을 붙이면 피지컬 address

translation을 할때 맨 먼처 쳐다보는건 PLB 
PLB가 있으면 physicadcl address
없으면 한번 더 과정을 한다

physical addres는 cash data를 가져온다.

리드는 콜드미스를 부른다.
라이트는 무조건 읽을필요가 없다. 메모리 데이터를 언젠가 나중에 디스크에 저장하면 된다.
ㅣㄺ은데이터를 가지고 할 경우 동기적으로 
쓰기는 모든 데이터를 discript에 하기 전에 비동기 쓰기 작업이 이루어져있다.
PAGE 32
dma는 direct memory access를 의미한다.

cpu 데이터 처리라고 하면 레지던스사이즈 한번에 32,64바이트로 한다
기가바이트 cpu를 멈추고, 다른 장치도 멈추고
Bus에서 데이터를 사용하지 않는 공간을 모아서 사용하는 방법을
DMA라고 한다

DMA : 내가 읽을 데이터를 가져왔는데 cpu캐시에는 쳐다보지 않을 수 있다.
이 동기화를 os가 맞춰야 한다
DMA가 overwrite할때 , 메모리 데이터를 한다 ?


페이지 데이터의 엔트리가 캐쉬가 반응하는지 안하는지를 비트필드한다
없으면 캐쉬가 반응하도록 설정한다

PAGE 36

파일시스템 크게 두가지
하나 디렉토리 구조 
둘 인덱스 구조

디렉토리구조와 인덱스 구조로 구분해있다

파일을 저장소에 저장하고 어떤파일인지 구분하는 소프트웨어 : 파일 시스템
이름을 붙이고 이름을 붙여서 데이터를 구분하고 어디에 저장하는지 

디렉토리 구조 :  흔히 폴더랑 비슷하지만 상세하게는 오늘 배운다
인덱스 구조 : 파일이 실제로 어디에 저장되어있는지

사용자가 데이터를 저장하는데 
데이터와 meta-data를  분리되서 파일이 이루어진다.
파일의 위치, 파일 사이즈 , 읽기 가능한지 / 쓰기 가능한지 등등이 메타데이터이다
데이터를 접근하기 위해 설명하는 데이터 : 메타데이터이다

디렉토리 구조 , 파일 인덱스 구조 : 모두 메타데이터이다

파일 이름을 가지고 파일 number을 찾는다 : 디렉토리 구조 
파일 number을 가지고 storage block을 찾는다 : index structre

파일 open -> 이름을 가지고 파일에 해당하는 파일 number를 찾는다. : open의 연산과정
프로세스가 실제가 open한 파일 데이터를 가지고 있다. read / write , sync등을 할 수 있다.

Direcotries : os에서 최근에 folder을 쓰기도 한다.
사전의미 - 전화번호부 , 목록이라고 나온다
디렉토리도 결국 파일이다. 해당하는 디렉토리에 있는 파일의 이름, 파일의 번호가 적혀져있다.
파일의 리스트가 같은 계층에 있는 목록을 볼 수 있다.

계층적 구조로 되어 있따. - 한 디렉토리 안에 다른 디렉토리를 만들 수 있다.
디렉토리 파릴이 또 있다면 하위 디렉토리에 또 다른 디렉토리 정보를 볼 수 있다

디렉토리 entry : 여러개가 들어있다.

엔트리는 파일과 디렉토리로 구분이 가능하다
엔트리는 이름, attribute, id, file number을 가지고 있다.
중요한 attribute는 중요한 자료구조로 따로 때논 attribute도 있다.

disk operation system : 파일시스템이 dos를 하게 할수있다
fat - 파일 annotation table의 약자
디렉토리 파일에다 추가적ㅈ인 attribute를 전부 넣어놓았다

파일 : 저장소 (저장소에 이름을 붙여넣은것)'
data와 metadata가 들어있다. metadata는 
디렉토리는 사람을 위해서 존재한다.
file id, filenumber을 가지고 os는 파일여부를 판단한다.
metadata - 파일의 소유자 , accesss-writer을 구분을 가지고 있다.
file이 어디를 읽고 있는지 도 메타데이터(open된 파일의 position) 

FAT : 테이블형태
디스크 오퍼레이션 시스템 : 
하드디스크를 가운데 동그랗게 생각해서 , 플랙/섹터/디스크로 구분한다.
1문에 n번 돌리는 디스크 : nrpm이라고 한다.
요즘 하드웨어는 논리적으로 일렬로 되어있따고 가정한후 제일 밖의 제일 끝에 있는 데이터를 보여준다.

tract-sector로 바꾼다.
controller로 인해서 01,2,3,4 등 index로 이어져있는 저장장치가 이어져있다.

테이블의 갯수가 나머지 뒤 데이터의 블록에 저장한 것과 동일하다
1:1 mapping이 되어야 한다.

디텍토리 - 파일이름을 가지고 번호를 찾는다. 
그 번호를 찾으면 hello.word filed의 디렉토리에서 찾게 되고, filenumber에 의해 찾는 범위가 달라진다.

링크드리스트 형식으로 데이터가 한블록 이상되면 그 다음 데가 어디에 들어있는지 데이터가 적혀져있다.

1. 파일 오픈 -> 2. 디렉토리 파일넘버 
fat : 링크드 리스트 형식으ㄹ 되어있다 ***

File은 disk block을 묵여있ㄱ 각 데이터를 block list 시작이 첫번째 노드가 된다.


dist에서 안쓰는 영역을 
FAT에서 

FAT 블록단위로 쪼개져 사용이 되면 파일들 읽기 쓰기를 마구잡이로 하면 
기존의 사용하고 있는 영역들이 있어서 그 기 존들의 영역을 버리고 새로운 경우에 쓰는 경우가 많았으나 (자투리 공간이 많아짐)
디스크` 정리 알고리즘 **유틸리티

Free영역을 한곳에 모아주고 s나머지도 한곳에 모아준다.
31
파일시스템 자체가 복자바지는 않다.

FAT는 제일 처음에 부티앟ㄹ때 FAT내용을 메모리에서 가져온다. 
포렌식의 경우 : 데이터의 블록을 직접 부르기도 한다.

단점 :::


링크드리스트 형태로 되어있어 링크드 리스트를 따라가면서 파일을 읽어야=하는데 
시퀀설을 할 수 가 없다.



시퀀설을 쪼개져있으니 불편하다

프리리스트가 리스트형태로 되어있어서 

피팅 알고리즘의 효율이 좋지 않다.
큰파일 access를 하게되면 list 트리버스 해야하는데 전부하가에 버퍼링이 걸린다.

리스트 만들때 동작 가능하게 하고 실제로 처음 사용하는 부분이 FAT이다.

PAGE47
디렉토리는 파일 이름, number, 이렇게 생겼다.
FAT에서는 여기서 추가적인 정보도 넣을 수 있게 생겼다.
시작하는 파일이 대게는 정해져 있다. (root파일)


Link 와 Direcotries -> 상대적으로 덜 중요

File access 만 만들고 디렉토리 관련된건 안한거같은데.. 심볼릭 링크도 안한거같구..

리눅스에서 특히 많이 사용되는데 
링크파일 : 이름은 다르지만 똑같은 파일을 만든다.
내용은 같은데 이름은 다른 방법

파일이 어디에 저장되어있는지알수 있는 방법 :
index structrre를 똑같이 가리키게 하는 방법(1) hard-link
파일의 이름은 다르지만 번호가 똑같아요(2) soft-link

a파일에 대한 path를 가지고 100번 파일이라고 기억할수있고
b파일에 대한 path를 가지고 100번 파일이라고 할 수 있게 한다


Big Fat Security holes :
index와 똑같은 number를 가리키기 때문에 index number를 훼손하면 Security holes도 망가진다

Git hub 예제

*** FAT는 링크드 리스트기반으로 하는 파일시스템이다 ***
unix 파일시스템

PAGE52

통계자료 
가로 : 파일의 사이즈 
세로 : 파일의 개수

상대적으로 파일 사이즈가 작은 부분들이 많다 : 2K
두번쩨 
가로 : containing file size
세로 : 파일시스템에서 사용하는 공간

: 전체적으로 사용되는 파일들이 많다
PAGE 53


파일의 번호에 따라서  array의 index를 찾아서 inode= 인덱스 번호가 나온다.
작은파일의 경우 몇번 블록을 사용하고 있는지
큰 파일들을 acces하기 위해 : two = level -> 하나의 second level 무언가 나오온것처럼
큰파일들은 inderection을 한번 거쳐서 블록 넘버들을 가지고올수있도록 (예 1000 블록) - 포인트를 가지고 작은파일들으 access할 수 있다
1000개 이상되면 dobule indirection을 한다
1000개가 하나의 indirectiondmf 한다. 1000 x 1000 을 해서 백만개 블록이 나온다.

마찬가지로 트리플 indirection을 할 수 있다.
가지고 온 다음에 사용자에게 맵핑한다

블록사이즈가 4k로 하고 한 블록이 가지는 포인트 들을 4byite
4키로 블록에 4바이트 포인트 : 1000개
4메가 바이트 > 4키로바이트 > 4테라바이트 이렇게 큰 단위로 할 수 있다.

unix의 파일 시스템es2 . es3
현재는 ex54가 많이 사용되고 있다.

cent os 명령어
mount 명령어를 통해 ext4 버전을 쓸 수 있다는 것을 알 수 있다.

최근의 db는 파일시스템을 사용한다
실제 남은 공간을 확인하려면 df -hard
파티션 / 전체 얼마나 쓸 수 있고 / 얼만큼 남아있는지를 알 수 있다.

*****파티션 키워드 검색하기 ****

각 파일들별 사이즈 - direct별 파일들을 계산할 수 있다.

ㅇdisk univalization

du . -sh : 실제로 파일들별로 사용하는 디스크 용량을 알 수 있다.


free: 실제 메모리 사용량을 알 수 있다.


inode는 os가 인식하는 index
실제로 예제를 드ㄴ건 4k / 4byite 
실제로는 조금 더 작은 사이즈로 되어있고 extint로 되어있ㄱ 시작번호부터 끝번호 / 시작번호부터 사이즈로 구성되어있다


FAT 파일 시스템 예전 디렉토리 파일에 메타데이터를 가지고 있다.
이제는 index node를 통해 많이 파일을 본다

근처에 있는 파일들을 한꺼번에 쭉 밀거나 , 한꺼번에 쭉 불러온다
hash로 최적화 할 수 있다

실제 file systeam에
pintos : src/filesys/file.cash
핀토스 - 스텐포드에서 만든 교육용 os
다른 사람이 내 파일을 못볼수있게 할 수 있다

UFS
user에 대해서 write권한을 빼면 read-only로 할 수 있다.
UGO
U - USer
G : 같은 그룹
O -others
X : 실행가능한지 불가능한지 알수있는 bit

111 : 7 
user에 대해서 7
read /write / exeucte
600 /644 : 

6 -read,write
4  - read x , write X

execute permition을 주면 된다.

SRAM
NAND gate를 가지고 비트를 트랩을 시켜 만든다.
한 비트를 저장할 수있다
8비트를 만들어 1바이트를 만든다

cashe : 는 정말 비싼 칩이다

DRAM
전기를 저장시킬수있는 캠퍼스터를 사용
볼티지를 넣는가 빼는가를 가지고 만든다
성능은 떨어진다. 직접 만들 수 있다

cashe는 cpu 안에 넣어야해서 용량보다는 속도가 중요하다 : SRAM을 사용한다. 표현을 $로 많이 사용한다.

Locality (spartial, temporial)
캐쉬 라인 : 같이 사용할 수 있는 아이들을 사용한다 
캐쉬 안에서 데이터를 찾을 수 있다면 hit 없다면 miss
hit가 많으면 많을수록 

메모

Mamory access time : 수십 nanocec , cache access time : 수백 , picosec 
SSD : 수십 us , HHD : 십 ms ,
속도 차이 : hit을
TLB : Page table translation 결과를 caching

context switching overhead가 크다
- CPU에서 메모리 access 를 하면 VI 를 하면 PL B - > PA (hit)
                                            -> page table lookup (table walk) : hit 하지 못하면 계속 왓다갓다 하면서 찾아야 한다.
PA -> cache -> memory

File system
pathname -> **directory structure -> File number
: 사용자를 위해
File number -> **index structre -> storage location
: OS가 인식하는 파일 , 저장소 내의 위치정보를 기억
search - FAT 를 예시로 들 수 있다.

FAT (file allocation table)
Liked list(연결리스트) 기반의 index structure(파일 정보 저장)
FAT 또다른 형태는 Block 과 1 대 1로 매치되는 FAT 테이블이 디스크 제일 앞에 위치


Unix Filel system
Unix fs (UFS)

파일 시스템은 링크드리스트라서 데이터가 연결되어 있어 연결된 데이터를 찾아서 추적한다
반면 Unix fs는 작은 파일과 큰 파일의 빠를 서치와 접근이 가능하도록 만들어졌다

작은 파일은 index , 큰 파일은 inderection 블록, dobule inderection블록, 트리플 inderection블록 등등으로 되어있다.

inode 하나가 운영체제에서는 한 파일이다.
root direcotry 파일에 대해서 파일넘버는 몇이다가 파일시스템에 미리 알려져 있다.
1번 파일 에는 루트 디렉토리에 있는 이름과 파일번호가 저장되어있다

inode 1번 root direcotry file number이다
1번 inode 정보


Direct pointers : 작은 파일 - 몇번 디스크 파일을 읽으면 되는지 알 수 있다 다음 next pointer가 몇인지 보고 바로 데이터 access가 this block을 통해 가능하다
indirect pointer - - injection

에고를 버리자

unix file system : fee block매 블럭에 bit를 체크 할 수 있다 0을찾는 instructre을 사용한다

CPU를 위해 os운영체제가 제공하는기능
타이머
프로텍트모드, user mode
어토믹 인스트렉션
FLB
find fist zero


FFS
장접
큰 파일 , 작은파일 모두 지원
빨리 찾을 수 있다
metadata, data를 묶어서 쓴다 -> inode의 같은/ 디렉토리에 있는 inode와 실제 date를 근처에 배치해서 많이 왓다갓다 하지 않게 한다
단점  :
짧은 파일에 비효울적 ()
파일이 이어져있는 / 연속적으로 분포되어있는 12345678910 이렇게 쭉 써야한다
10`20%를 항상 free space로 둬야 한다.

파일이름이 string으로 되어 있어 hash함수를 가지고 디렉토리에 넣어서 하나하나 lookup하지 않고 B-Tree를 사용해서 파일을 찾을 수 있다
윈도우 98 FAT기반
윈도우 2000 : NT (윈도우에 nt라는 버전이 있었다. 이 버전을 일반인에게 사용할 수 있도록 했다. 파일이 정말 작으면 마스터파일에 적을 수 있게 했다)
variable contigus 에서 여러 블록을 한번에 표현할 수 있도록 : Extents 방법을 사용한다 (현제 exts4, 5버전에서도 쓴다)

hex : 16진수로 dump한 파일
fs.h : 이미지 해석 파일
disk.img : 바이너리 파일

REadfile is cotcat

1.디렉터리 를 가지고 filenumber 로 변경
2. filenmber를 가지고 실제 disk를 가지고 위치를 읽어야 한다


https://github.com/mobile-os-dku-cis-mse/mse-os-proj3
파일 이름을 파일넘버로 가지고, index-structrre
index-structrre를 가지고 실제 block에 접근하는 과제

앞부분 metadata : 뒤에 1zlfh qkdlxm 데이터들이 들어가있다

super_block : 1키로 바이트를 맞췄다
마추기 위해 heading를 맞췄다

첫번째 inode = first_inode = root
4바이트,

pattion_type , first_node : 4qkdlxm /4qkdlxm /4바이트/4바이트 

32 byte가 indoe 32바이트
short형은 2바이트 , 여러사람이 파일 엑세를 방지하기 위해 락
아이노드 : 쇼츠 이바이트 6개 총 12 + 나머지 = 32바이트
실제 번호가 5개 적허있ㄷ. 그 번호가 해당하는 디스크넘버를 찾아가면 정보가 있다
스트럭트 블록 1키로로 디1024

전체 팔키로바이트 사이즈 : 8키로 바이트르 제외한 나머지 마지막줄

파일을 열면
목표는
파일의 내용을 읽는다

파일의 내용을 읽으려면 ㅇ=루트파일을 읽어야 한다

루트파일의 ㅂ번호는 슈퍼블록의 퍼스트 아이노드에 적혀있다


0. find root file - 슈퍼블록
1. root file read
2.https://github.com/mobile-os-dku-cis-mse/mse-os-proj3 named file

루트디렉퇴레 있는 파일들의 목록들이 읽을 수 있다. 파일원의 파일들이 있을때 검증할때 루ㅡ트파이릐 내용을 읽는다
파일 1이 있는 파일 넘버를 찾고 그 파일넘버에 있는 부분을 찾아야 한다.


hex파일을 누르면 view raw를 누르면 번호들이 보인다.
앞부분에 써져있는 내용 중에서 4/4/4/4바이트 중 4번째 -> firstinode가 적혀있다
8글자가 4byite이다.

data블록이 3번데이터블록을 읽으면 삼번 아이노드를 가서 보면 된다.
삼번 아이노드에 가서 보면 (아이노드가 있는 지역 400~)

3번 inode : 두번째줄 0000470 와 그 윗줄
블록 번호는 끝의 12바이트를 읽으면 된다
0470으로 시작하는 0400
앞의 4바이트는 인다이렉트 블록
사이즈는 000000028
hex data로 28이니까 계산기로 계산해보면
16진수로 28이니까 40사이즈다

4번 블록에서 찾을 수 있다



2000 부터 0번블록 시작
4블록이면 4키로가 증가,

400바이트씩 끊어서 하면 데이터블록 4번은 300부터 시작하는것을 알 수있다
00000300은 40바이트 



***************
***************
hex number 
바이트
인다이렉션
이 부분을 좀 더 공부해보기
***************
***************

파일시스템을 읽는 과정 : 14시~15시 어려워서 멍...
지금까지 루트 디렉토리에 있는 파일 1번을 읽었다.

파일 깊이가 한층씩 올라갈수록 위 과정을 1번씩 더 한다.

최근의 데이터들은 copy on write로 한다
고친것을 나중에 반영하는 방식이다

처음에는 데이터를 fork하면 vm이 동일한 데이터가 생긴다
실제로 write를 할때 메모리의 복사를 밀어둔다
(똑같은 데이터를 복사하는것을 중복되므로 최대한 밀어둔다)

PAGE 74
고치는 부분에서만 수정을 한다. 옛날데이터는 그대로 두고
옛날에두는것보다 새로운 부분을 쓰는게 더 좋으면
새로운 부분을 사용한다(초록색)

시스템의 여유가 있어서 변경되는 부분을 반영하는게 ㅔ될때
천천히 업데이트를 반영해도 된다.
멀티코어가 나오면서 전체 파일을 lock을 결고 한명만 업데이트 하는것보다 부분부분 lock을 걸고 부분부분 수정하는것도 괜찮다.

저글링 파일 시스템이라고도 부른다
최근의 ssd 기반의 파일시스템은 저널링 기반의 파일 시스템을 많이 사용한다 
데이터를 한꺼번에 쭉 올린다. file operation을 하는 경우가 많다

memory mapped File
스토리지에 쓰는 파일을 메모리에 가져온다.
프로세스 1번이 돌면 리드 시스템 콜을 했다.
커널이 프로세스 1번은 다 읽을때까지 
스케쥴에서 1번 프로세스를 뺀다

그 사이에 디스크에 있는 데이터를 읽어온다면?

프로세스마다 자신이 가지고 있는 address-space는 다르고
커널은 공유하는 형태로 되어있다
address-space가 생기고 실제로 사용자가 돌아가는 낮은 주소공간은 다른 프로세스를 사용하고 있는 형태
사용자가 자기 주소 공간에 있지 않다
일번 프로세스가 가져갈 수 있는 방법이 아예 없다.

현제 ㅔ실제로 active하게 돌아가는 공간은 2번 프로세스이다.
이렇게 주어진 파일을 1번 프로세스가 가져갈수 있도록 
memory로 copy를 하거나 데이터가 가지고 있는 주소를 랩핑하면서 해결할 수 있다

nmap : memory mapping files 하는 과정
자기가 읽을 디스크의 파일의 공간과 랩핑하고, 그 메모리를 헤더 프로세스로 사용할 수 있다.
(읽기를 좀 더 쉽게 하는 과정)

memory address 상의 region을 골라서 쓴다.
안드로이드 os간에 process : 바인더 과정
(ex 핸드폰 알람 홈 스크린과 메시지 통신) 을 nmaped 파일 통신으로 한다.

// Log-structured file system : Buffer cache & page cache
디스크 데이터를 계산을 디스크에 하는 것보다 메모리에 하는것이 좋다
모든 디스크의 메모리를 다가져 와서 쓸 수 없다.
디스크를 쓴다고 하면 메모리를 가지고 와서 쓴다. 메모리에 아까 읽어왔던 데이터를 cpu에서 사용한다

unix에는 buffer cache라는 이름을로 관리를 하다가 리눅스에서 page cache라는 이름으로
사용자가 직접 다루는 캐쉬와 메타데이터 캐쉬를 구분한다
읽어오는 data 중 inode를 buffer cache 에 저장한다
buffer cache에 저장되지 않으면 데이터가 유실될수있고 파일시스템 자체에 문제가 생긴다.
inode자체가 망가진다
buffer cache 와 page cache를 비워주는 역할을 꺼내기 역할에서 한다

page cache 영향이 실제로는 buffer cache보다 더 크다.


I/O devices and Networking

렉
40줄 정도 가 한줄로 되어있다. 
장치별로 속도차이가 많이 나서 pci bus에 전부 올리지 않았다
펜텀2 

10의 12승 속도차이가 난다 : 네트워크 
버스 토폴로지 : 이 선을 모든 장치가 공유하는 경우로 된다
아무리 빨른 것이 몰라도 속도가 빠른아이가 정말 많이 난다
I/O 장치
최근에는 실제로 컴퓨터랑 비슷하게 나왔다
Controller라고 부르는 아이가 cpu역할을 하고
memory를 buffer라고도 한다
SSD : 실제 핸드폰에 들어가는 cpu가 들어간다
핸드폰에 들어가는 cpu도 arm이라는 ssd가 들어간다


네트워크카드하고 cpu는 각자 동작한다.
메모리와 cpu가 각각 다른것처럼

비동기적으로 실행된다
I/O장치들은 cpu하고 비동기적으로 실행한다.

디스크의 데이터를 읽어달라고 한다면, 디스크의 장치가 I/O 컨트롤러가 작동
멀티스레드 기법을 활용해서 cpu 작업과 I/O작업으로 분리한다. 각각 역할에 집중하고 역할을 번갈아가면서한다. (동시에 실행한다)
쓰레드 레벨의 페럴리즘 : 성능최적화 기법을 최근에 많이 연구한다

DMA Controller는 I/O controller에서 한다. 해당하는 쓰레드를 맵핑하던지 카핑하던지 cpu에서 한다

어떻게 디바이스와 커뮤니케이션을하는가?
명령어로 쓰는 I/O : 라즈베리파이, 인텔 등등 = i/0 instructions
cpu를 멈추고 i/o 하라. (읽고 쓰는 부분이 따로)

고성능을 사용하는 i/o는 memory mapped i/o를 사용한다
매번 명령어를 내리지 않고 장치를 일할 수 있을때 커맨드 버퍼에 할일을 적어놓고 커맨드 버퍼에서 할일을 맡아서 작동한다

주소 엑세스에 디스크를 옮겨라 하면 메모리 엑세스에는 버퍼가 있고 DMA 컨트롤러
DMA컨ㄹ트롤러가 disk 컨트롤러에 전달
DMA컨트롤러가 비는 시간을 본 다음 커맨드를 날린다.
디스크 컨트롤러가 바이트를 DMA에 날린다.
전송이 끝나면 cpu에 신호를 보낸다. (데이터가 전부 날라갔다 라고 메시지를 날려~)

I/o interrupt 와 polling
폴링은 디바이스 상태가 무엇인지 직접 체크하고
인터럽트는 인터럽트ㅡㄹㄹ 받은 다음 해다아는 스레드가 실행하는 보장이 없다
인터러브는 cpu에 오버해더가 적다 cpu활용측면에서 인터럽트가 유용

폴링 : 초고식 네트워크에서 자주 사용된다 - 주고 받는 부분이 빠르기 때문이다.

operationl parmeter for I/o

내일은 네트워크와 클라우드컴퓨터~~






















