# 선형 회귀 (Linear Legression)

![통계학에서는 좀 어려운 표현으로 이 선을 최적선, 영어로는 line of best fit이라고도 합니다.  집 크기가 주어졌을 때, 이걸 이용해서 집 값을 예측하는 예시를 사용할게요. 선형 회귀는 여기 있는 데이터를 가장 잘 대변해 주는 선을 찾아내는 것입니다. 이 데이터에 가장 잘 맞는, 가장 적절한 하나의 선을 찾아내는 거죠.](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled.png)

통계학에서는 좀 어려운 표현으로 이 선을 최적선, 영어로는 line of best fit이라고도 합니다.  집 크기가 주어졌을 때, 이걸 이용해서 집 값을 예측하는 예시를 사용할게요. 선형 회귀는 여기 있는 데이터를 가장 잘 대변해 주는 선을 찾아내는 것입니다. 이 데이터에 가장 잘 맞는, 가장 적절한 하나의 선을 찾아내는 거죠.

### 변수

### 목표 변수 = target variable = output variable = 아웃풋

### 입력 변수 = input variable = feature

![나중에는 좀 더 복잡해지겠지만, 일단 우리가 찾으려는 선은 어떤 곡선이 아니라 그냥 직선입니다. 직선이라는 건 일차 함수라는 거고, 그러면 y = ax + b의 형태로 표현됩니다. 결국 선형 회귀의 임무는 계수 a랑 상수 b를 찾아내는 거죠.](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%201.png)

나중에는 좀 더 복잡해지겠지만, 일단 우리가 찾으려는 선은 어떤 곡선이 아니라 그냥 직선입니다. 직선이라는 건 일차 함수라는 거고, 그러면 y = ax + b의 형태로 표현됩니다. 결국 선형 회귀의 임무는 계수 a랑 상수 b를 찾아내는 거죠.

!['가설 함수', 영어로는 'hypothesis function'이라고 부릅니다.](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%202.png)

'가설 함수', 영어로는 'hypothesis function'이라고 부릅니다.

![dfs 여기서 \theta_0*θ*0는 상수항이기 때문에 곱해지는 입력 변수가 없는데요. 그렇기 때문에 통일성을 위해서 이렇게 표현할 때도 있습니다.  이 식에서는 항상 x_0 = 1*x*0=1 이렇게 고정을 해놓은 건데요. 혹시 이렇게 돼있는 표현을 보면 그냥  \theta_0*θ*0은 상수항이라고 생각하시면 됩니다.](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%203.png)

dfs 여기서 \theta_0*θ*0는 상수항이기 때문에 곱해지는 입력 변수가 없는데요. 그렇기 때문에 통일성을 위해서 이렇게 표현할 때도 있습니다.  이 식에서는 항상 x_0 = 1*x*0=1 이렇게 고정을 해놓은 건데요. 혹시 이렇게 돼있는 표현을 보면 그냥  \theta_0*θ*0은 상수항이라고 생각하시면 됩니다.

![ 어떤 기준을 두고 비교하는 게 좋겠죠?](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%204.png)

 어떤 기준을 두고 비교하는 게 좋겠죠?

# **평균 제곱 오차**

가설 함수가 얼마나 좋은지 평가하는 방법을 알려드리겠습니다. 선형 회귀에서 가장 많이 쓰는 방법은 평균 제곱 오차, 영어로는 mean squared error라는 것입니다. 앞 글자만 따서 MSE라고도 하는데, 그냥 MSE라고 쓸게요.

이 평균 제곱 오차라는 건, 이 데이터들과 가설 함수가 평균적으로 얼마나 떨어져 있는지 나타내기 위한 하나의 방식인데요. 계산하는 방법을 보여드릴게요.

이렇게 데이터가 있고 가설 함수가 있으면, 이 데이터들이 각각 이 선에서 어느정도씩 벗어나잖아요? 좀 더 정확하게 표현하자면, 각 데이터의 실제 값과, 이 가설 함수가 예측하는 값이 조금씩 차이가 나는 건데요.

### 

![집의 크기 47평을 가설 함수에 넣으면 집 값이 18.8억으로 예측됩니다. 하지만 이 집의 실제 가격은 22억네요. 오차는 예측 값에서 원래 값을 뺀 건데요. -3.2죠?  집의 크기 39평을 가설 함수에 넣으면 가격 이 15.6억으로 예측되는데, 실제 가격은 9억입니다. 그러면 오차가 6.6인 거죠. 이런 식으로 오차를 다 구할 수 있는데, 이 오차값들을 모두 제곱합니다. 그리고 제곱한 값들을 더합니다. 그리고 이것의 평균을 내기 위해서 총 데이터 개수 만큼 나누면 됩니다.](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%205.png)

집의 크기 47평을 가설 함수에 넣으면 집 값이 18.8억으로 예측됩니다. 하지만 이 집의 실제 가격은 22억네요. 오차는 예측 값에서 원래 값을 뺀 건데요. -3.2죠?  집의 크기 39평을 가설 함수에 넣으면 가격 이 15.6억으로 예측되는데, 실제 가격은 9억입니다. 그러면 오차가 6.6인 거죠. 이런 식으로 오차를 다 구할 수 있는데, 이 오차값들을 모두 제곱합니다. 그리고 제곱한 값들을 더합니다. 그리고 이것의 평균을 내기 위해서 총 데이터 개수 만큼 나누면 됩니다.

### 평균 제곱 오차가 크다는 건 가설 함수와 데이터들 간의 오차가 크다는 거고, 결국 그 가설 함수는 데이터들을 잘 표현해 내지 못한 겁니다.

### ****제곱을 하는 이유****

![양수로 통일을 할 수 있습니다. 오차에 제곱을 하는 또 다른 이유는, 오차가 커질수록 더 부각시키기 위해서입니다](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%206.png)

양수로 통일을 할 수 있습니다. 오차에 제곱을 하는 또 다른 이유는, 오차가 커질수록 더 부각시키기 위해서입니다

### ****평균 제곱 오차 일반화****

![마지막에 그 합을 총 개수인 m으로 나눠서 평균을 내면 되는 겁니다.](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%207.png)

마지막에 그 합을 총 개수인 m으로 나눠서 평균을 내면 되는 겁니다.

![h는 가설 함수입니다. 그리고 당연히 x는 인풋, y는 아웃풋인데요. x^{(i)}x 
 는 i번째 인풋, y^{(i)} 는 i번째 아웃풋을 뜻하는 겁니다. 집 가격 예측 프로그램을 
예시로 들면, x^{(i)}는 i번째 집의 크기고 y^{(i)}는 i번째 집의 가격이겠죠.](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%208.png)

h는 가설 함수입니다. 그리고 당연히 x는 인풋, y는 아웃풋인데요. x^{(i)}x 
 는 i번째 인풋, y^{(i)} 는 i번째 아웃풋을 뜻하는 겁니다. 집 가격 예측 프로그램을 
예시로 들면, x^{(i)}는 i번째 집의 크기고 y^{(i)}는 i번째 집의 가격이겠죠.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%209.png)

이건 시그마라고 하는 그리스 문자입니다. 시그마 밑에 "i는 1"이라고 되어 있고, 위에 m이라는 문자가 있습니다. 이게 어떤 의미냐면, i에 1부터 m까지 반복적으로 대입하라는 겁니다. 시그마 오른쪽에 있는 부분을 보면 이렇게 i가 있잖아요? i에 1을 대입해서 계산하고, i에 2를 대입해서 계산하고, i에 3을 대입해서 계산하고... 이런 식으로 i에 1부터 m까지 대입해서 계산하는 건데요. 그리고 나서 이렇게 계산된 값들을 모두 더해야 합니다. 계산된 값들을 다 더하는 것까지가 이 시그마의 역할입니다.

### 손실함수

선형회귀는 값을 잘 맞는 최적선을 구하는 과정이다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2010.png)

잘 맞는 선을 찾는 것을 하나하나 찾을때 그 선을 가설함수라고 한다. 

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2011.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2012.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2013.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2014.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2015.png)

![손실 함수는 보통 J라는 문자를 쓰고요... 선형 회귀의 경우에는 평균 제곱 오차가 손실 함수의 아웃풋입니다. 특정 가설 함수의 평균 제곱 오차가 크면 이 손실 함수의 아웃풋이 크다는 거고, 그러면 손실이 크기 때문에 안 좋은 가설 함수라는 거죠. 반대로 가설 함수의 평균 제곱 오차가 작으면 이 손실 함수의 아웃풋이 작다는 거고, 그러면 손실이 적기 때문에 좋은 가설 함수인 겁니다.](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2016.png)

손실 함수는 보통 J라는 문자를 쓰고요... 선형 회귀의 경우에는 평균 제곱 오차가 손실 함수의 아웃풋입니다. 특정 가설 함수의 평균 제곱 오차가 크면 이 손실 함수의 아웃풋이 크다는 거고, 그러면 손실이 크기 때문에 안 좋은 가설 함수라는 거죠. 반대로 가설 함수의 평균 제곱 오차가 작으면 이 손실 함수의 아웃풋이 작다는 거고, 그러면 손실이 적기 때문에 좋은 가설 함수인 겁니다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2017.png)

### ****왜 인풋이 세타인가?****

가설 함수에서 바꿀 수 있는 건 이 세타 값들이죠? 세타 값들을 잘 조율해서, 가장 적합한 가설 함수를 찾아내는 건데요. 그러면 결국 손실 함수의 아웃풋은 이 세타 값들을 어떻게 설정하느냐에 달려 있는 겁니다. 그래서 손실 함수의 인풋이 세타인 거죠. 반면 x랑 y는 변수처럼 보이지만 사실 정해진 데이터를 대입하는 것이기 때문에, 얘네들이 손실 함수에서는 오히려 변수가 아니라 상수라고 할 수 있습니다.

### 최적선의 기준은 손실함수로 구한다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2018.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2019.png)

### 손실함수의 아웃풋을 최소화 하는 방법을 경사하강법이라고 한다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2020.png)

### 

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2021.png)

### 

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2022.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2023.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2024.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%20(Linear%20Legression)%2044ff39b8fe9e4815915e067c2235a322/Untitled%2025.png)