# 로지스틱 회귀(Logistic Regression)

### 머신러닝은 지도학습과 비지도학습으로 나뉜다.

### 지도학습은 회귀와 분류로 나뉜다.

회귀는 연속적인 값 예측, 분류는 정해진 몇개의 값 중 예측

분류 예시 : 스팸인지 일반이메일인지 / 스포츠 -정치 -연예 신문내용중 어떤신문?

### 선형회귀는 예외적인 데이터 분류에 민감하다

선형회귀는 데이터에 가장 잘 맞는 일차함수를 찾는다.

반면, 로지스틱회귀는 데이터에 가장 잘 맞는 시그모이드를 찾는다. 

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled.png)

```
*x*가 엄청 커서 무한대라고 합시다. 시그모이드 식에서 x*x*에 무한대를 대입하면, 
e*e*의 마이너스 무한대 제곱이잖아요? e*e*는 2.718이라는 양수입니다. 
정확히 뭔지는 일단 모르셔도 됩니다. 2.718의 마이너스 무한대 제곱이면 뭐가 나올까요? 0이 나옵니다. 
그러면 분모가 1 + 0이니까 그냥 1이 돼서, 이 함수의 결괏값은 1분의 1... 그냥 1이 됩니다.

```

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%201.png)

## 시그모이드 함수는 무조건 0과 1사이의 값을 리턴한다.

결과가 0과 1사이의 의미란?

선형회귀는 결과가 범위 없이 얼마든지 크거나 작아질 수 있음

무조건 0과 1사이의 값이 나오는 시그모이드 함수가 분류에 더 적합하다.

### 리턴 값이 결국 0과 1 사이의 연속적인 값 ⇒

### 시그모이드 결과값이 0.5보다 큰지 작은지로 분류한다 !

### 가설함수

입력 변수를 받아서 목표변수를 예측해주는 함수

![선형 회귀에서는 가설 함수가 이렇게 생겼습니다. 벡터를 사용하면 더 간결하게 표현할 수 있겠죠.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%202.png)

선형 회귀에서는 가설 함수가 이렇게 생겼습니다. 벡터를 사용하면 더 간결하게 표현할 수 있겠죠.

### 선형회귀를 약간 변형하여 로지스틱 함수를 받을 수 있다.

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%203.png)

### 

### 

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%204.png)

### 

![함수 g는 일차 함수죠? 일차 함수는 아웃풋이 엄청 클 수도 있고, 엄청 작을 수도 있습니다. 예를 들어서 일차 함수가 이런 직선이라면, 인풋이 엄청 커지거나 엄청 작아지면 결국 아웃풋도 엄청 커지거나 엄청 작아질 수 있겠죠?](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%205.png)

함수 g는 일차 함수죠? 일차 함수는 아웃풋이 엄청 클 수도 있고, 엄청 작을 수도 있습니다. 예를 들어서 일차 함수가 이런 직선이라면, 인풋이 엄청 커지거나 엄청 작아지면 결국 아웃풋도 엄청 커지거나 엄청 작아질 수 있겠죠?

![그런데 로지스틱 회귀를 할 때 우리는 아웃풋이 항상 0과 1 사이에 떨어지도록 하고 싶습니다. 그러기 위해서는 '시그모이드 함수'라는 걸 쓰면 되는데요. 이 시그모이드가 하는 역할이 딱 그겁니다. 여기에 어떤 인풋을 넣든 간에, 아웃풋은 무조건 0과 1 사이에 떨어집니다. 선형 회귀에서 썼던 가설 함수인 g*g*의 아웃풋을, 이 시그모이드 함수의 인풋으로 집어 넣으면, 결국 0과 1 사이의 결과가 나오겠죠?  참고로 0이나 1이 나오는 게 아니라, 0과 1 사이의 어떤 수가 나오는 겁니다. 유의해 주시길 바랍니다.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%206.png)

그런데 로지스틱 회귀를 할 때 우리는 아웃풋이 항상 0과 1 사이에 떨어지도록 하고 싶습니다. 그러기 위해서는 '시그모이드 함수'라는 걸 쓰면 되는데요. 이 시그모이드가 하는 역할이 딱 그겁니다. 여기에 어떤 인풋을 넣든 간에, 아웃풋은 무조건 0과 1 사이에 떨어집니다. 선형 회귀에서 썼던 가설 함수인 g*g*의 아웃풋을, 이 시그모이드 함수의 인풋으로 집어 넣으면, 결국 0과 1 사이의 결과가 나오겠죠?  참고로 0이나 1이 나오는 게 아니라, 0과 1 사이의 어떤 수가 나오는 겁니다. 유의해 주시길 바랍니다.

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%207.png)

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%208.png)

### ****속성이 하나일 때****

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%209.png)

예를 들어서 공부 시간으로 시험을 통과했는지 탈락했는지 예측하고 싶다고 할게요. 그럼 속성이 하나니까 이렇게 가설 함수가 있겠죠? 모든 데이터에 대해서 파란색 영역에 있으면, 통과, 빨간색 영역에 있으면 탈락이라고 할 수 있겠죠? 이렇게 분류를 할 때, 분류를 구별하는 경계선을 Decision Boundary라고 부릅니다.

### ****속성이 2개일 때****

> 모의고사 시험 점수를 갖고 시험을 통과했는지를 예측한다고 할게요. 공부 시간을 x_1 모의고사 시험 점수를 x_2 라고 했을 때  가설 함수
> 

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2010.png)

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2011.png)

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2012.png)

## 로그 손실

### 로그 손실이라고 부르는 이유는, 손실의 정도를 로그 함수로 결정하기 때문

### 손실 함수 : 가설함수를 평가한다.

로지스틱 회귀의 손실 함수는 평균 제곱 오차를 사용하지는 않습니다. 대신 '로그 손실', 영어로는 log loss라는 걸 사용합니다. 좀 더 어려운 표현으로는 cross entropy라고도 합니다.

### 

![여기서 h(x)h(x)는 어떤 입력 변수에 대한 가설 함수의 예측값이죠? yy는 실제 값이고요. 로그 손실 함수는 예측값이 실제 결과랑 얼마나 괴리가 있는지 알려 주는 역할을 합니다.

그런데 로지스틱 회귀는 분류 알고리즘이잖아요? 그리고 분류가 두 가지라고 가정하면, 가능한 목표 변수가 1과 0밖에 없습니다. 이 두 경우에 대해서 식이 조금 다른 건데요.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2013.png)

여기서 h(x)h(x)는 어떤 입력 변수에 대한 가설 함수의 예측값이죠? yy는 실제 값이고요. 로그 손실 함수는 예측값이 실제 결과랑 얼마나 괴리가 있는지 알려 주는 역할을 합니다.

그런데 로지스틱 회귀는 분류 알고리즘이잖아요? 그리고 분류가 두 가지라고 가정하면, 가능한 목표 변수가 1과 0밖에 없습니다. 이 두 경우에 대해서 식이 조금 다른 건데요.

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2014.png)

### 실제 아웃풋이 1일때

![만약 h(x)가 0.8 정도면, 80%의 확률로 아웃풋이 1일 거라고 예측하는 건데, 실제 결과가 1이기 때문에 이 가설 함수는 꽤 잘했다고 평가할 수 있습니다. 손실이 좀 있긴 하지만, 그렇게 크진 않죠.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2015.png)

만약 h(x)가 0.8 정도면, 80%의 확률로 아웃풋이 1일 거라고 예측하는 건데, 실제 결과가 1이기 때문에 이 가설 함수는 꽤 잘했다고 평가할 수 있습니다. 손실이 좀 있긴 하지만, 그렇게 크진 않죠.

![, 왼쪽으로 갈수록 손실이 커지는데, 처음에는 슬슬 커지다가 급격하게 가파라집니다. h(x)가 1에서 멀어질수록 잘 못하고 있는 거니까, 손실을 엄청나게 키우는 거죠.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2016.png)

, 왼쪽으로 갈수록 손실이 커지는데, 처음에는 슬슬 커지다가 급격하게 가파라집니다. h(x)가 1에서 멀어질수록 잘 못하고 있는 거니까, 손실을 엄청나게 키우는 거죠.

### 실제 아웃풋이 0일때

![그래프가 거꾸로 되는 겁니다. h(x))가 0이라는 건, 아웃풋이 1일 확률이 0%라고 예측하는 건데, 실제 결과가 0이기 때문에 완벽하게 예측했다고 할 수 있습니다. 그래서 손실이 0인 거죠.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2017.png)

그래프가 거꾸로 되는 겁니다. h(x))가 0이라는 건, 아웃풋이 1일 확률이 0%라고 예측하는 건데, 실제 결과가 0이기 때문에 완벽하게 예측했다고 할 수 있습니다. 그래서 손실이 0인 거죠.

![만약 h(x)가 0.2 정도면, 20%의 확률로 아웃풋이 1일 거라고 예측하는 건데, 실제 결과가 0이기 때문에 나쁘지 않게 예측했다고 할 수 있습니다. 그래서 손실이 좀 있긴 하지만 크진 않습니다.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2018.png)

만약 h(x)가 0.2 정도면, 20%의 확률로 아웃풋이 1일 거라고 예측하는 건데, 실제 결과가 0이기 때문에 나쁘지 않게 예측했다고 할 수 있습니다. 그래서 손실이 좀 있긴 하지만 크진 않습니다.

![오른쪽으로 갈수록 손실이 커지고, 처음에는 슬슬 커지다가 그래프가 급격하게 가파라집니다. h(x)가 0에서 멀어질수록 못하고 있는 거라서, 손실을 엄청나게 키우는 겁니다.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2019.png)

오른쪽으로 갈수록 손실이 커지고, 처음에는 슬슬 커지다가 그래프가 급격하게 가파라집니다. h(x)가 0에서 멀어질수록 못하고 있는 거라서, 손실을 엄청나게 키우는 겁니다.

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2013.png)

실제 아웃풋 y*y*가 1인 경우에는 예측값이 1에 가까울수록 손실이 0에 가깝고, 멀어질수록 손실이 가파르게 커집니다. 예측값이 1에서 멀어질수록 더 틀린 거니까 더 많은 페널티를 주는 거죠.

실제 아웃풋 y*y*가 0인 경우에는 정반대입니다. 예측값이 0에 가까울수록 손실이 0에 가깝고, 멀어질수록 손실이 가파르게 커집니다. 예측값이 0에서 멀어질수록 더 틀린 거니까 더 많은 페널티를 주는 거죠.

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2020.png)

### y가 1일때

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2021.png)

### y가 0일때

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2022.png)

![로지스틱 회귀의 손실 함수를 쓸 때는 보통 이 한 줄 방식을 사용합니다.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2023.png)

로지스틱 회귀의 손실 함수를 쓸 때는 보통 이 한 줄 방식을 사용합니다.

![로그 손실 함수가 있죠? 로그 손실을 이용해서 우리의 i번째 데이터에 대한 손실을 구하는 겁니다.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2024.png)

로그 손실 함수가 있죠? 로그 손실을 이용해서 우리의 i번째 데이터에 대한 손실을 구하는 겁니다.

![그런데 i는 뭘까요? 그게 이 시그마에서 나오는 건데요. 시그마 밑에 i=1이라고 되어 있고, 위에는 m이라고 되어 있습니다. m은 학습 데이터 개수죠. 여기 오른쪽 부분에서 i에 1을 대입하고, 2를 대입하고, 3을 대입하고... 이런 식으로 i에 1부터 m까지 순서대로 대입을 해서 계산하고, 계산된 결과를 모두 더하는 겁니다. 다 더하는 것까지가 이 시그마의 역할입니다](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2025.png)

그런데 i는 뭘까요? 그게 이 시그마에서 나오는 건데요. 시그마 밑에 i=1이라고 되어 있고, 위에는 m이라고 되어 있습니다. m은 학습 데이터 개수죠. 여기 오른쪽 부분에서 i에 1을 대입하고, 2를 대입하고, 3을 대입하고... 이런 식으로 i에 1부터 m까지 순서대로 대입을 해서 계산하고, 계산된 결과를 모두 더하는 겁니다. 다 더하는 것까지가 이 시그마의 역할입니다

![. 마지막에 m으로 나눠 주면 평균을 구할 수 있는 거고요.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2026.png)

. 마지막에 m으로 나눠 주면 평균을 구할 수 있는 거고요.

### 간단하게 요약하자면, 모든 학습 데이터에 대해서 로그 손실을 계산하고, 평균을 내는 겁니다. 그걸로 가설 함수를 평가하는 거죠.

![가설 함수는 세타 값들을 어떻게 설정하느냐에 따라 바뀐다. 어떤 세타 값들을 설정하느냐에 따라 학습 데이터의 손실이 달라지는 겁니다. 그래서 손실 함수의 인풋은 세타인 거죠.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2027.png)

가설 함수는 세타 값들을 어떻게 설정하느냐에 따라 바뀐다. 어떤 세타 값들을 설정하느냐에 따라 학습 데이터의 손실이 달라지는 겁니다. 그래서 손실 함수의 인풋은 세타인 거죠.

### ****로그 손실 대입****

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2028.png)

### 로지스틱 회귀 경사 하강법

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2029.png)

![처음에는 세타 값들을 모두 0으로 지정하거나 모두 랜덤하게 지정합니다. 어디선가는 시작을 해야 되니까요. 그러면 현재 세타 값들에 대한 손실, 즉 현재 가설 함수에 대한 손실을 계산할 수 있는데요. 여기서부터 시작해서, 세타를 조금씩 조율하면서 손실을 계속 줄여나가야 하는 겁니다.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2030.png)

처음에는 세타 값들을 모두 0으로 지정하거나 모두 랜덤하게 지정합니다. 어디선가는 시작을 해야 되니까요. 그러면 현재 세타 값들에 대한 손실, 즉 현재 가설 함수에 대한 손실을 계산할 수 있는데요. 여기서부터 시작해서, 세타를 조금씩 조율하면서 손실을 계속 줄여나가야 하는 겁니다.

### 편미분

![손실 함수를 세타0에 대해서 편미분하고, 그 결과에 학습률 알파를 곱합니다. 그리고 그 결과를 기존 *θ*0에서 빼면 되는 겁니다. *θ* 1이랑 똑같이 *θ*2도 업데이트하면 되는 거죠. 이렇게 모든 세타값들을 업데이트하면, 경사 하강을 **한 번** 했다고 할 수 있습니다. 그리고 이걸 충분히 반복하면 손실을 최소화할 수 있는 거죠.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2031.png)

손실 함수를 세타0에 대해서 편미분하고, 그 결과에 학습률 알파를 곱합니다. 그리고 그 결과를 기존 *θ*0에서 빼면 되는 겁니다. *θ* 1이랑 똑같이 *θ*2도 업데이트하면 되는 거죠. 이렇게 모든 세타값들을 업데이트하면, 경사 하강을 **한 번** 했다고 할 수 있습니다. 그리고 이걸 충분히 반복하면 손실을 최소화할 수 있는 거죠.

![이건 선형 회귀든 로지스틱 회귀든 똑같이 사용할 수 있는데요. 차이점은 이 손실 함수 J가 다르다는 것입니다. 지금 우리는 로지스틱 회귀를 하는 거니까, J에 로지스틱 회귀의 손실 함수를 대입해 봅시다.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2032.png)

이건 선형 회귀든 로지스틱 회귀든 똑같이 사용할 수 있는데요. 차이점은 이 손실 함수 J가 다르다는 것입니다. 지금 우리는 로지스틱 회귀를 하는 거니까, J에 로지스틱 회귀의 손실 함수를 대입해 봅시다.

![세타값이 많아지면 식도 계속 늘어나니까, 일반화해서 이렇게 표현할게요. *j*에 0을 넣어서 세타0을 업데이트 하고, j에 1을 넣어서 세타1을 업데이트 하고, j에 2를 넣어서 세타2를 업데이트 하고... 이런 식으로 하면 됩니다.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2033.png)

세타값이 많아지면 식도 계속 늘어나니까, 일반화해서 이렇게 표현할게요. *j*에 0을 넣어서 세타0을 업데이트 하고, j에 1을 넣어서 세타1을 업데이트 하고, j에 2를 넣어서 세타2를 업데이트 하고... 이런 식으로 하면 됩니다.

![이렇게 선형 회귀랑 거의 똑같은데, 여기서 유일하게 다른 건 이 가설 함수 h입니다. 선형 회귀에서의 가설 함수는 그냥 일차 함수였는데, 로지스틱 회귀에서의 가설 함수는 시그모이드 함수죠? *h*에 이 시그모이드 함수를 대입해서 각 세타값을 업데이트하면 되는 겁니다.](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2034.png)

이렇게 선형 회귀랑 거의 똑같은데, 여기서 유일하게 다른 건 이 가설 함수 h입니다. 선형 회귀에서의 가설 함수는 그냥 일차 함수였는데, 로지스틱 회귀에서의 가설 함수는 시그모이드 함수죠? *h*에 이 시그모이드 함수를 대입해서 각 세타값을 업데이트하면 되는 겁니다.

### ****입력 변수와 파라미터 표현****

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2035.png)

### ****모든 데이터 예측 값****

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2036.png)

로지스틱 회귀는 선형 회귀랑 가설 함수 *hθ*(*x*) 가 다르잖아요?

위에서 계산한 모든 결과 값을 시그모이드 함수에 넣어줘야 되는데요. 계산한 *Xθ*의 모든 원소를 시그모이드 함수에 넣는다는 의미로 이렇게 표현할게요.

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2037.png)

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2038.png)

### ****예측 오차****

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2039.png)

모든 예측 값들과 목표 변수의 차이를 간단하게 표현할 수 있는 거죠. 뒤에서는 표현하기 쉽게 이 값을 error라고 부르겠습니다.

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2040.png)

그렇기 때문에 경사 하강법도 다중 선형 회귀를 할 때랑 똑같이 표현해줄 수 있습니다.

위에서 했던 거처럼 입력 변수, 파라미터, 예측값 오차를 이렇게 표현할 때:

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2041.png)

![Untitled](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20a2087baa54ee424b8d8b7cfff5d041fd/Untitled%2042.png)